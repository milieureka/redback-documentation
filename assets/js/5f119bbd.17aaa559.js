"use strict";(self.webpackChunkredback_documentation=self.webpackChunkredback_documentation||[]).push([[6991],{26306:(r,e,n)=>{n.r(e),n.d(e,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var t=n(74848),i=n(28453);const o={sidebar_position:1},a="Crowd Monitoring Overview",s={id:"project-4/Crowd-Monitoring/Crowd-Monitoring-Overview",title:"Crowd Monitoring Overview",description:"The Project Orion aims to apply cutting-edge AI technology to enhance crowd monitoring at various levels. In this semester, we are building on previous efforts to develop an intelligent tracking system, moving from IoT approach to application approach. Our main goal is create a cloud-based computer vision pipeline for improved efficiency and faster processing. Later, the application can connect to a database platform and fetching real-time data.",source:"@site/docs/project-4/Crowd-Monitoring/Crowd-Monitoring-Overview.md",sourceDirName:"project-4/Crowd-Monitoring",slug:"/project-4/Crowd-Monitoring/Crowd-Monitoring-Overview",permalink:"/redback-documentation/docs/project-4/Crowd-Monitoring/Crowd-Monitoring-Overview",draft:!1,unlisted:!1,editUrl:"https://github.com/Redback-Operations/redback-documentation/blob/main/docs/project-4/Crowd-Monitoring/Crowd-Monitoring-Overview.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"IOT Guided Documents",permalink:"/redback-documentation/docs/category/iot-guided-documents-1"},next:{title:"Project 5",permalink:"/redback-documentation/docs/category/project-5"}},c={},l=[{value:"YOLOv8",id:"yolov8",level:2},{value:"Visualization",id:"visualization",level:2},{value:"Results",id:"results",level:2}];function p(r){const e={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",img:"img",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...r.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h1,{id:"crowd-monitoring-overview",children:"Crowd Monitoring Overview"}),"\n",(0,t.jsxs)(e.p,{children:["The Project Orion aims to apply cutting-edge AI technology to enhance crowd monitoring at various levels. In this semester, we are building on previous efforts to develop an intelligent tracking system, moving from IoT approach to application approach. Our main goal is create a ",(0,t.jsx)(e.strong,{children:"cloud-based"})," computer vision pipeline for improved efficiency and faster processing. Later, the application can connect to a database platform and fetching real-time data."]}),"\n",(0,t.jsx)(e.h2,{id:"yolov8",children:"YOLOv8"}),"\n",(0,t.jsx)(e.p,{children:"Computer Vision (CV) is used in traffic anlysis, automation of manufacturing processed,and human monitoring, which is the essential aspect that we are focusing this semester."}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"YOLOv8"})," is a state of the art to monitor and track people in real-time. By combine that with Supervision library, we can detect and track people."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.img,{alt:"YOLOv8 performance",src:n(24827).A+"",width:"3840",height:"1440"}),"\r\nSource: ",(0,t.jsx)(e.a,{href:"https://docs.ultralytics.com/models/yolov8/",children:"https://docs.ultralytics.com/models/yolov8/"})]}),"\n",(0,t.jsx)(e.h1,{id:"blue-print",children:"Blue print"}),"\n",(0,t.jsx)(e.p,{children:"We are focusing to build a pipeline for real-time camera process."}),"\n",(0,t.jsx)(e.p,{children:"CCTV >> YOLOv8 >> MongoDB >> Website/Dashboard"}),"\n",(0,t.jsx)(e.h1,{id:"initialize-libraries",children:"Initialize libraries"}),"\n",(0,t.jsxs)(e.p,{children:["This tells you the versions of both PyTorch and CUDA that are installed for ",(0,t.jsx)(e.strong,{children:"Environment setup"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\r\n!nvcc --version\r\nTORCH_VERSION = ".".join(torch.__version__.split(".")[:2])\r\nCUDA_VERSION = torch.__version__.split("+")[-1]\r\nprint("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)\n'})}),"\n",(0,t.jsx)(e.p,{children:"We will use YOLOv8 in this project:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"!pip install ultralytics\r\n\r\nfrom IPython import display\r\ndisplay.clear_output()\r\n\r\nimport ultralytics\r\nultralytics.checks()\n"})}),"\n",(0,t.jsx)(e.p,{children:"Supervision library:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'!pip install supervision==0.2.0\r\n\r\nfrom IPython import display\r\ndisplay.clear_output()\r\n\r\nimport supervision as sv\r\nprint("supervision", sv.__version__)\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import os\r\nHOME = os.getcwd()\r\nprint(HOME)\n"})}),"\n",(0,t.jsx)(e.h1,{id:"testing-crowd-monitoring",children:"Testing Crowd Monitoring"}),"\n",(0,t.jsx)(e.p,{children:"We will use video from Supervision assets - PEOPLE_WALKING"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.a,{href:"https://media.roboflow.com/supervision/video-examples/people-walking.mp4",children:"https://media.roboflow.com/supervision/video-examples/people-walking.mp4"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:"After downloaded, you will need to import into to your directory if using on Google Colab."})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Video testing with YOLOv8 model"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Importing Libraries\r\nfrom ultralytics import YOLO, solutions\r\nfrom ultralytics.solutions import object_counter\r\nimport cv2\r\nimport os\r\nimport numpy as np\r\nfrom IPython.display import display, Image\r\n\r\n# Define the video path - Use your own path\r\nMARKET_SQUARE_VIDEO_PATH = "/content/people-walking.mp4"\r\n\r\n# Open the video file\r\ncap = cv2.VideoCapture(MARKET_SQUARE_VIDEO_PATH)\r\nassert cap.isOpened(), "Error reading video file"\r\n\r\n# Load the YOLO model\r\nmodel = YOLO("yolov8n.pt")\r\n\r\n# Verify the output directory and permissions\r\noutput_dir = "/content"\r\nif not os.path.exists(output_dir):\r\n    os.makedirs(output_dir)\r\n\r\nif not os.access(output_dir, os.W_OK):\r\n    raise PermissionError(f"Write permission denied for the directory {output_dir}")\r\n\r\n# Define the output video path\r\noutput_path = os.path.join(output_dir, "Peoplewalking_v8_29July.mp4")\r\n\r\n# Reading the Video\r\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\r\n\r\n# Initialize VideoWriter with a successful FourCC code\r\nfourcc_code = cv2.VideoWriter_fourcc(*"mp4v")\r\nvideo_writer = cv2.VideoWriter(output_path, fourcc_code, fps, (w, h))\r\nif not video_writer.isOpened():\r\n    raise IOError(f"Error initializing video writer with path {output_path}")\r\n\r\n# Assigning the points for Region of Interest\r\nregion_points = [(20, 1000), (1080, 1000), (1080, 2000), (20, 2000)]\r\n\r\n# Initialize the ObjectCounter with the model\'s class names\r\ncounter = solutions.ObjectCounter(\r\n    view_img=True,\r\n    reg_pts=region_points,\r\n    names=model.names,\r\n    draw_tracks=True,\r\n    line_thickness=2,\r\n)\r\n\r\nwhile cap.isOpened():\r\n    success, im0 = cap.read()\r\n    if not success:\r\n        print("Video frame is empty or video processing has been successfully completed.")\r\n        break\r\n    tracks = model.track(im0, persist=True, show=False, imgsz=1280)\r\n\r\n    im0 = counter.start_counting(im0, tracks)\r\n    video_writer.write(im0)\r\n\r\ncap.release()\r\n\r\nvideo_writer.release()\r\ncv2.destroyAllWindows()\r\nprint(f"Processed video saved to {output_path}")\n'})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Camera testing with YOLOv8"})}),"\n",(0,t.jsx)(e.p,{children:"--Waiting for input--"}),"\n",(0,t.jsx)(e.h2,{id:"visualization",children:"Visualization"}),"\n",(0,t.jsx)(e.p,{children:"We will need to visualize data to display and analysis on dashboard."}),"\n",(0,t.jsxs)(e.blockquote,{children:["\n",(0,t.jsxs)(e.p,{children:["The idea is fetching the tracking path from processed data, plotting their points from ",(0,t.jsx)(e.strong,{children:"camera footage"})," onto ",(0,t.jsx)(e.strong,{children:"2D floor plan"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:["From that, we can do ",(0,t.jsx)(e.strong,{children:"flow analysis"})," and ",(0,t.jsx)(e.strong,{children:"heatmap"})," to evaluate crowd density."]}),"\n",(0,t.jsx)(e.p,{children:"This is a powerful information. It will allow you to easily recognize common pattern of congestion at particular times of day or places. Moreover, it can improve your business performance by arranging staffs and products, make inform decisions to drive sales."}),"\n",(0,t.jsx)(e.p,{children:"Your security camera images are distorted. For example, a one pixel movement at the top of your image corresponds to a much larger movement in the real world than a one pixel movement at the bottom of your image."}),"\n",(0,t.jsxs)(e.blockquote,{children:["\n",(0,t.jsx)(e.p,{children:"Homography Transformation is the solution for camera mapping."}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.img,{alt:"Homography Transformation",src:n(91981).A+"",width:"1244",height:"530"}),"\r\n",(0,t.jsx)(e.img,{alt:"Homography Transformation",src:n(60214).A+"",width:"1234",height:"474"}),"\r\nSource: ",(0,t.jsx)(e.a,{href:"https://zbigatron.com/mapping-camera-coordinates-to-a-2d-floor-plan/",children:"https://zbigatron.com/mapping-camera-coordinates-to-a-2d-floor-plan/"})]}),"\n",(0,t.jsx)(e.p,{children:"We need to calculate corresponse mapping matrix H for homography transformation. We can create the matrix by choosing pixel coordinates in camera view and corresponding pixel coordinates in matching image (at least 4 points)."}),"\n",(0,t.jsxs)(e.blockquote,{children:["\n",(0,t.jsx)(e.p,{children:"Use matrix H to performed track points transformation to plot path on map 2D floor plane."}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{alt:"Matrix Transformation",src:n(14015).A+"",width:"1238",height:"498"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"TESTING"})}),"\n",(0,t.jsx)(e.p,{children:"Fetching and draw track path of camera view"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\nfrom ultralytics import YOLO\r\nfrom collections import defaultdict\r\n\r\n# Initialize YOLO model\r\nmodel = YOLO("yolov8n.pt")\r\n\r\n# Open video file\r\nvideo_path = "/content/people-walking.mp4"\r\ncap = cv2.VideoCapture(video_path)\r\n\r\n# Initialize track history\r\ntrack_history = defaultdict(list)\r\n\r\n# Initialize video writer (optional, if you want to save the output)\r\nfourcc = cv2.VideoWriter_fourcc(*"mp4v")\r\nvideo_writer = cv2.VideoWriter(\'/content/tracking_white_output.mp4\', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\r\n\r\n# Assigning the points for Region of Interest\r\nregion_points = [(20, 500), (1080, 500), (1080, 1000), (20, 1000)]\r\n\r\n# Initialize the ObjectCounter with the model\'s class names\r\n# Init Object Counter\r\ncounter = solutions.ObjectCounter(\r\n    view_img=True,\r\n    view_in_counts\t= True,\r\n    view_out_counts\t= True,\r\n    reg_pts=region_points,\r\n    names=model.names,\r\n    #draw_tracks=True,\r\n    line_thickness=2,\r\n)\r\nwhile cap.isOpened():\r\n    success, frame = cap.read()\r\n    if not success:\r\n        break\r\n    # Get the frame dimensions\r\n    height, width, channels = frame.shape\r\n\r\n    # Create a white frame of the same size\r\n    white_frame = np.ones((height, width, channels), dtype=np.uint8) * 255\r\n\r\n    # Track objects in the frame\r\n    results = model.track(frame, persist=True, show=False, imgsz=1280, verbose=True)\r\n\r\n\r\n    # Extract tracking results\r\n    boxes = results[0].boxes.xywh.cpu().numpy()\r\n    track_ids = results[0].boxes.id.int().cpu().numpy()\r\n\r\n    # Draw paths for each track\r\n    #frame_with_counting = counter.start_counting(frame, results)\r\n    #annotated_frame = frame_with_counting.copy()  # Make a copy of the frame to draw paths on\r\n    annotated_frame = white_frame  # Make a copy of the frame to draw paths on\r\n    for track_id in np.unique(track_ids):\r\n        # Get the history for this track_id\r\n        history = track_history[track_id]\r\n        if len(history) > 1:\r\n            points = np.array(history, dtype=np.int32)\r\n            # Draw the path (line connecting the points)\r\n            cv2.polylines(annotated_frame, [points], isClosed=False, color=(0, 255, 0), thickness=2)\r\n\r\n    # Update the track history with new positions\r\n    for box, track_id in zip(boxes, track_ids):\r\n        x, y, w, h = box\r\n        center = (int(x), int(y + h / 2))\r\n        track_history[track_id].append(center)\r\n        # Limit history length\r\n        if len(track_history[track_id]) > 50:\r\n            track_history[track_id].pop(0)\r\n\r\n    # Save or display the frame\r\n    video_writer.write(annotated_frame)\r\n\r\n\r\n# Release resources\r\ncap.release()\r\nvideo_writer.release()\r\ncv2.destroyAllWindows()\n'})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Draw floor plan"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from google.colab.patches import cv2_imshow\r\nimport numpy as np\r\n\r\ndef create_floor_replica(canvas_height, canvas_width, num_tiles_x, num_tiles_y):\r\n    """\r\n    Create a floor replica with a white canvas and rectangular tiles.\r\n\r\n    Parameters:\r\n    canvas_height (int): Height of the canvas.\r\n    canvas_width (int): Width of the canvas.\r\n    num_tiles_x (int): Number of tiles horizontally.\r\n    num_tiles_y (int): Number of tiles vertically.\r\n\r\n    Returns:\r\n    floor_image (numpy.ndarray): The generated floor image with tiles.\r\n    """\r\n    # Calculate the height of each tile\r\n    tile_height = canvas_height // num_tiles_y\r\n    # Calculate the width of each tile\r\n    tile_width = canvas_width // num_tiles_x\r\n\r\n    # Create a white canvas\r\n    floor_image = np.ones((canvas_height, canvas_width, 3), dtype=np.uint8) * 255\r\n\r\n    # Draw the tiles (rectangles)\r\n    for y in range(0, canvas_height, tile_height):\r\n        for x in range(0, canvas_width, tile_width):\r\n            cv2.rectangle(floor_image, (x, y), (x + tile_width, y + tile_height), (0, 0, 0), 1)\r\n\r\n    return floor_image\r\n\r\n# Example usage\r\nif __name__ == "__main__":\r\n    # Define the canvas size (height and width)\r\n    canvas_height = 1000  # Example height\r\n    canvas_width = 700  # Example width\r\n\r\n    # Number of tiles horizontally and vertically\r\n    num_tiles_x = 25\r\n    num_tiles_y = 15\r\n\r\n    # Create the floor replica\r\n    floor_image = create_floor_replica(canvas_height, canvas_width, num_tiles_x, num_tiles_y)\r\n\r\n    # Display the result\r\n    cv2_imshow(floor_image)\r\n    cv2.imwrite(\'/content/floor_replica.png\', floor_image)\r\n    cv2.waitKey(0)\r\n    cv2.destroyAllWindows()\n'})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Perform transformation and draw tracking path on floor plan"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\nfrom ultralytics import YOLO\r\nfrom collections import defaultdict\r\n\r\n# Initialize YOLO model\r\nmodel = YOLO("yolov8n.pt")\r\n\r\n# Open video file\r\nvideo_path = "/content/people-walking.mp4"\r\ncap = cv2.VideoCapture(video_path)\r\n\r\n# Initialize track history\r\ntrack_history = defaultdict(list)\r\n\r\n# Load the base image\r\nbase_image_path = \'/content/floor_replica.png\'\r\nfloor_frame = cv2.imread(base_image_path)\r\n\r\n# Ensure the base image is loaded\r\nif floor_frame is None:\r\n    raise ValueError(f"Could not load the base image from {base_image_path}")\r\n\r\n# Get the dimensions of the base image\r\nheight, width, channels = floor_frame.shape\r\n\r\n# Initialize video writer with the dimensions of the base image\r\nfourcc = cv2.VideoWriter_fourcc(*"mp4v")\r\nvideo_writer = cv2.VideoWriter(\'/content/2D_map_output.mp4\', fourcc, 20.0, (width, height))\r\n\r\n# Matching points from 2 views\r\n# Provide points from image 1\r\npts_src = np.array([[28, 1158], [2120, 1112], [1840, 488], [350, 518], [468, 1144]])\r\n# Corresponding points from image 2\r\npts_dst = np.array([[0, 990], [699, 988], [693, 658], [0, 661], [141, 988]])\r\n\r\n# Define homography functions\r\ndef calculate_homography(pts_src, pts_dst):\r\n    return cv2.findHomography(pts_src, pts_dst)[0]\r\n\r\ndef transform_points(points, homography_matrix):\r\n    points = np.concatenate([points, np.ones((points.shape[0], 1))], axis=1)  # Add a column of ones for homogenous coordinates\r\n    transformed_points = homography_matrix.dot(points.T).T  # Apply homography\r\n    transformed_points /= transformed_points[:, 2][:, np.newaxis]  # Normalize by the third coordinate\r\n    return transformed_points[:, :2]\r\n\r\n# Calculate the homography matrix once, since pts_src and pts_dst are constant\r\nhomography_matrix = calculate_homography(pts_src, pts_dst)\r\n\r\n# Process each frame\r\nwhile cap.isOpened():\r\n    success, frame = cap.read()\r\n    if not success:\r\n        break\r\n\r\n    # Track objects in the frame\r\n    results = model.track(frame, persist=True, show=False, imgsz=1280, verbose=True)\r\n\r\n    # Extract tracking results\r\n    boxes = results[0].boxes.xywh.cpu().numpy()\r\n    track_ids = results[0].boxes.id.int().cpu().numpy()\r\n\r\n    # Use a fresh copy of the base image\r\n    annotated_frame = floor_frame.copy()\r\n\r\n    for track_id in np.unique(track_ids):\r\n        # Get the history for this track_id\r\n        history = track_history[track_id]\r\n        if len(history) > 1:\r\n            points = np.array(history, dtype=np.int32)\r\n            # Transform the points using the precomputed homography matrix\r\n            new_points = transform_points(points, homography_matrix)\r\n            new_points = new_points.astype(np.int32)\r\n            # Draw the path (line connecting the points)\r\n            cv2.polylines(annotated_frame, [new_points], isClosed=False, color=(0, 255, 0), thickness=2)\r\n\r\n    # Update the track history with new positions\r\n    for box, track_id in zip(boxes, track_ids):\r\n        x, y, w, h = box\r\n        center = (int(x), int(y + h / 2))\r\n        track_history[track_id].append(center)\r\n        # Limit history length\r\n        if len(track_history[track_id]) > 50:\r\n            track_history[track_id].pop(0)\r\n\r\n    # Save the annotated frame to the video\r\n    video_writer.write(annotated_frame)\r\n\r\n# Release resources\r\ncap.release()\r\nvideo_writer.release()\r\ncv2.destroyAllWindows()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"results",children:"Results"}),"\n",(0,t.jsx)(e.p,{children:"input GIF"})]})}function d(r={}){const{wrapper:e}={...(0,i.R)(),...r.components};return e?(0,t.jsx)(e,{...r,children:(0,t.jsx)(p,{...r})}):p(r)}},91981:(r,e,n)=>{n.d(e,{A:()=>t});const t=n.p+"assets/images/image-1-393c61fea90fa9628c8995e3f479d9e9.png"},60214:(r,e,n)=>{n.d(e,{A:()=>t});const t=n.p+"assets/images/image-2-bf21c11ef9bafc24005a0081a43d4728.png"},14015:(r,e,n)=>{n.d(e,{A:()=>t});const t=n.p+"assets/images/image-3-432759eec5f3249b070260c8dca5fdb7.png"},24827:(r,e,n)=>{n.d(e,{A:()=>t});const t=n.p+"assets/images/image-b15e3dcfb08da2f41ee09f38dea0c8ba.png"},28453:(r,e,n)=>{n.d(e,{R:()=>a,x:()=>s});var t=n(96540);const i={},o=t.createContext(i);function a(r){const e=t.useContext(o);return t.useMemo((function(){return"function"==typeof r?r(e):{...e,...r}}),[e,r])}function s(r){let e;return e=r.disableParentContext?"function"==typeof r.components?r.components(i):r.components||i:a(r.components),t.createElement(o.Provider,{value:e},r.children)}}}]);